{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e4057f",
   "metadata": {},
   "source": [
    "# ***End to End tutorial for SMS_SPAM labeling using High Level Supervision:***\n",
    "**The paper and documentation can be found here:** [Paper](https://openreview.net/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf), [Documentation](https://spear-decile.readthedocs.io/en/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b88922",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "User don't need to include this cell to use the package\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd22ce",
   "metadata": {},
   "source": [
    "# ***Defining an Enum to hold labels:***\n",
    "### **Representation of class Labels**\n",
    "\n",
    "<p>All the class labels for which we define labeling functions are encoded in enum and utilized in our next tasks. Make sure not to define an Abstain(Labeling function(LF) not deciding anything) class inside this Enum, instead import the ABSTAIN object as used later in LF section.</p>\n",
    "\n",
    "<p>SPAM dataset contains 2 classes i.e <b>HAM</b> and <b>SPAM</b>. Note that the numbers we associate can be anything but it is suggested to use a continuous numbers from 0 to number_of_classes-1</p>\n",
    "\n",
    "<p><b>**Note that even though this example is a binary classification, this(SPEAR) library supports multi-class classification**</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d396484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# enum to hold the class labels\n",
    "class ClassLabels(enum.Enum):\n",
    "    SPAM = 1\n",
    "    HAM = 0\n",
    "\n",
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8759f9",
   "metadata": {},
   "source": [
    "# ***Defining preprocessors, continuous_scorers, labeling functions:***\n",
    "During labeling the unlabelled data we lookup for few keywords to assign a class SMS.\n",
    "\n",
    "<b>Example</b> : *If a message contains apply or buy in it then most probably the message is spam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93a5ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigWord1 = {\"free\",\"credit\",\"cheap\",\"apply\",\"buy\",\"attention\",\"shop\",\"sex\",\"soon\",\"now\",\"spam\"}\n",
    "trigWord2 = {\"gift\",\"click\",\"new\",\"online\",\"discount\",\"earn\",\"miss\",\"hesitate\",\"exclusive\",\"urgent\"}\n",
    "trigWord3 = {\"cash\",\"refund\",\"insurance\",\"money\",\"guaranteed\",\"save\",\"win\",\"teen\",\"weight\",\"hair\"}\n",
    "notFreeWords = {\"toll\",\"Toll\",\"freely\",\"call\",\"meet\",\"talk\",\"feedback\"}\n",
    "notFreeSubstring = {\"not free\",\"you are\",\"when\",\"wen\"}\n",
    "firstAndSecondPersonWords = {\"I\",\"i\",\"u\",\"you\",\"ur\",\"your\",\"our\",\"we\",\"us\",\"youre\"}\n",
    "thirdPersonWords = {\"He\",\"he\",\"She\",\"she\",\"they\",\"They\",\"Them\",\"them\",\"their\",\"Their\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ffb4b",
   "metadata": {},
   "source": [
    "### **Declaration of a simple preprocessor function**\n",
    "\n",
    "\n",
    "For most of the tasks in NLP, computer vivsion instead of using the raw datapoint we preprocess the datapoint and then label it. Preprocessor functions are used to preprocess an instance before labeling it. We use **`@preprocessor(name,resources)`** decorator to declare a function as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102adabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "\n",
    "@preprocessor(name = \"LOWER_CASE\")\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "lower = convert_to_lower(\"RED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddae1cb",
   "metadata": {},
   "source": [
    "# ***High Level Supervision Algorithm:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944fe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_data_feeders\n",
    "import collections\n",
    "\n",
    "f_d = 'f_d'\n",
    "f_d_U = 'f_d_U'\n",
    "test_w = 'test_w'\n",
    "\n",
    "train_modes = [f_d, f_d_U]\n",
    "\n",
    "F_d_U_Data = collections.namedtuple('GMMDataF_d_U', 'x l m L d r')\n",
    "F_d_Data = collections.namedtuple('GMMDataF_d', 'x labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be11f7",
   "metadata": {},
   "source": [
    "### **Importing the required functionalities**\n",
    "\n",
    "\n",
    "Import the required libraries. Also, import the latest version of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2758a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from spear.Implyloss import *\n",
    "import numpy as np\n",
    "import sys, os, shutil\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898a182",
   "metadata": {},
   "source": [
    "### **Setting up the model's checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016a1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file does not exist\n",
      "INFO:tensorflow:best.ckpt-51 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-51\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:best.ckpt-52 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-52\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:best.ckpt-53 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:best.ckpt-54 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-54\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.266783/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.743327/best.ckpt-12 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.743327/best.ckpt-13 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.743327/best.ckpt-14 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.743327/best.ckpt-13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.743327/best.ckpt-13\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.743327/best.ckpt-15 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.743327/best.ckpt-13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.743327/best.ckpt-13\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.743327/best.ckpt-16 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.743327/best.ckpt-16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.743327/best.ckpt-16\n",
      "Best ckpt path:  /tmp/best_ckpt_0.743327/best.ckpt-15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.743327/best.ckpt-15\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints/hls-model\n",
      "Restoring checkpoint from path:  /tmp/checkpoints/hls-model\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints/hls-model\n",
      "Restoring checkpoint from path:  /tmp/checkpoints/hls-model\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints/hls-model\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.064679/hls-model-11\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.064679/hls-model-11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.064679/hls-model-11\n",
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/training/saver.py:968: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.064679/hls-model-5\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.064679/hls-model-5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.064679/hls-model-5\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.443770/hls-model-11\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.443770/hls-model-11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.443770/hls-model-11\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.443770/hls-model-5\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.443770/hls-model-5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.443770/hls-model-5\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "test_best_ckpt()\n",
    "test_checkmate()\n",
    "test_checkpoint()\n",
    "test_mru_checkpoints(num_to_keep=1)\n",
    "test_mru_checkpoints(num_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb2908",
   "metadata": {},
   "source": [
    "### **Initializing the Directories for storing relevant information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f76abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir =  './checkpoint'\n",
    "# data_dir = \"/home/parth/Desktop/SEM6/RnD/Learning-From-Rules/data/TREC\" # Directory containing data pickles\n",
    "# data_dir = \"/home/parth/Desktop/SEM6/RnD/spear/examples/SMS_SPAM/data_pipeline/\"\n",
    "data_dir = \"../../examples/SMS_SPAM/data_pipeline/\"\n",
    "inference_output_dir = './inference_output/'\n",
    "log_dir = './log/hls'\n",
    "metric_pickle_dir = './met_pickl/'\n",
    "tensorboard_dir =  './tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aec03",
   "metadata": {},
   "source": [
    "### **Creating the directories if they don't exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9f760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(inference_output_dir):\n",
    "    os.makedirs(inference_output_dir)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(metric_pickle_dir):\n",
    "    os.makedirs(metric_pickle_dir)\n",
    "\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b133fa9",
   "metadata": {},
   "source": [
    "### **Initializing the parameter values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbdc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load_mode = 'mru' # Which kind of checkpoint to restore from. Possible options are mru: Most recently saved checkpoint. Use this to continue a run f_d, f_d_U: Use these to load the best checkpoint from these runs \n",
    "# d_pickle = data_dir+\"d_processed.p\"\n",
    "d_pickle = data_dir+\"sms_pickle_L.pkl\"\n",
    "dropout_keep_prob =  0.8\n",
    "early_stopping_p = 20 # early stopping patience (in epochs)\n",
    "f_d_adam_lr =  0.0003 # default = 0.01\n",
    "f_d_batch_size = 16\n",
    "f_d_class_sampling = [10,10] # Comma-separated list of number of times each d instance should be sampled depending on its class for training f on d. Size of list must equal number of classes.\n",
    "f_d_epochs = 4 # default = 2\n",
    "f_d_metrics_pickle = metric_pickle_dir+\"metrics_train_f_on_d.p\"\n",
    "f_d_primary_metric = 'accuracy' #'f1_score_1' # Metric for best checkpoint computation. The best metrics pickle will also be stored on this basis. Valid values are: accuracy: overall accuracy. f1_score_1: f1_score of class 1. avg_f1_score: average of all classes f1_score \n",
    "f_d_U_adam_lr =  0.0003 # default = 0.01\n",
    "f_d_U_batch_size = 32\n",
    "f_d_U_epochs = 4 # default = 2  \n",
    "f_d_U_metrics_pickle = metric_pickle_dir+\"metrics_train_f_on_d_U.p\"\n",
    "f_infer_out_pickle = inference_output_dir+\"infer_f.p\" # output file name for any inference that was ran on f (classification) network\n",
    "gamma = 0.1 # weighting factor for loss on U used in implication, pr_loss, snorkel, generalized cross entropy etc. \n",
    "lamda = 0.1\n",
    "min_rule_coverage = 0 # Minimum coverage of a rule in U in order to include it in co-training. Rules which have coverage less than this are assigned a constant weight of 1.0.\n",
    "mode = \"implication\" # \"learn2reweight\" / \"implication\" / \"pr_loss\" / \"f_d\" \n",
    "test_mode = \"\" # \"\" / test_f\" / \"test_w\" / \"test_all\"\n",
    "num_classes = 2 # can be 0. Number of classes. If 0, this will be dynamically determined using max of labels in 'd'.\n",
    "num_load_d = None # can be 0. Number of instances to load from d. If 0 load all.\n",
    "num_load_U = None # can be 0. Number of instances to load from U. If 0 load all.\n",
    "num_load_validation = None # can be 0. Number of instances to load from validation. If 0 load all.\n",
    "q = \"1\"\n",
    "rule_classes = None # Comma-separated list of the classes predicted by each rule if string is empty, rule classes are determined from data associated with rule firings.\n",
    "shuffle_batches = True # Don't shuffle batches. Useful for debugging and stepping through batch by batch\n",
    "test_w_batch_size = 1000\n",
    "# U_pickle = data_dir+\"U_processed.p\"\n",
    "U_pickle = data_dir+\"sms_pickle_U.pkl\"\n",
    "use_joint_f_w = False # whether to utilize w network during inference\n",
    "# validation_pickle = data_dir+\"validation_processed.p\"\n",
    "validation_pickle = data_dir+\"sms_pickle_V.pkl\"\n",
    "w_infer_out_pickle = inference_output_dir+\"infer_w.p\" # output file name for any inference that was ran on w (rule) network\n",
    "json_file = data_dir+\"sms_json.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a0fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "output_dir = \"./\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if test_mode==\"\":\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir, ignore_errors=True)    \n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# number of input dir - 1 (data_dir)\n",
    "# number of output dir - 6 (checkpoint, inference_output, log_dir, metric_pickle, output, tensorboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769866e",
   "metadata": {},
   "source": [
    "### **Creating a Data Feeder Object to process data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41858faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/Desktop/SEM6/RnD/spear/notebooks/SMS_SPAM/../../spear/Implyloss/model.py:641: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:./checkpoint/f_d_U/best.ckpt-124 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "if(str(test_mode)==\"\"):\n",
    "    output_text_file=log_dir + \"/\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)+\".txt\"\n",
    "else:    \n",
    "    output_text_file=log_dir + \"/\" + str(test_mode) + \"_\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)+\".txt\"\n",
    "sys.stdout = open(output_text_file,\"w\")\n",
    "if(test_mode!=\"\"):\n",
    "    mode = test_mode\n",
    "if mode not in ['learn2reweight', 'implication', 'f_d', 'pr_loss', 'gcross',  'label_snorkel', 'pure_snorkel', 'gcross_snorkel', 'test_f', 'test_w', 'test_all']:\n",
    "    raise ValueError('Invalid run mode ' + mode)\n",
    "\n",
    "data_feeder = DataFeeder(d_pickle, \n",
    "                         U_pickle, \n",
    "                         validation_pickle,\n",
    "                         json_file,\n",
    "                         shuffle_batches, \n",
    "                         num_load_d, \n",
    "                         num_load_U, \n",
    "                         num_classes, \n",
    "                         f_d_class_sampling, \n",
    "                         min_rule_coverage, \n",
    "                         rule_classes, \n",
    "                         num_load_validation, \n",
    "                         f_d_batch_size, \n",
    "                         f_d_U_batch_size, \n",
    "                         test_w_batch_size,\n",
    "                         out_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92840867",
   "metadata": {},
   "outputs": [],
   "source": [
    " num_features, num_classes, num_rules, num_rules_to_train = data_feeder.get_features_classes_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fd1f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features: \", num_features)\n",
    "print(\"Number of classes: \",num_classes)\n",
    "print(\"Print num of rules to train: \", num_rules_to_train)\n",
    "print(\"Print num of rules: \", num_rules)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ca4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_classes = data_feeder.rule_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e973e0",
   "metadata": {},
   "source": [
    "### **Initializing the rule network and classification network of the algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3dc90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_network = networks.w_network_fully_connected #rule network - CHANGE config in w_network_fully_connected of my_networks - DONE\n",
    "f_network = networks.f_network_fully_connected #classification network - CHANGE config in f_network_fully_connected of my_networks - DONE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8dff5",
   "metadata": {},
   "source": [
    "### **Creating a High Level Supervision Network Object to be trained and tested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1882b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/home/parth/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "hls = HighLevelSupervisionNetwork(\n",
    "            num_features,\n",
    "            num_classes,\n",
    "            num_rules,\n",
    "            num_rules_to_train,\n",
    "            rule_classes,\n",
    "            w_network,\n",
    "            f_network,\n",
    "            f_d_epochs, \n",
    "            f_d_U_epochs, \n",
    "            f_d_adam_lr, \n",
    "            f_d_U_adam_lr, \n",
    "            dropout_keep_prob, \n",
    "            f_d_metrics_pickle, \n",
    "            f_d_U_metrics_pickle, \n",
    "            early_stopping_p, \n",
    "            f_d_primary_metric, \n",
    "            mode, \n",
    "            data_dir, \n",
    "            tensorboard_dir, \n",
    "            checkpoint_dir, \n",
    "            checkpoint_load_mode, \n",
    "            gamma, \n",
    "            lamda,\n",
    "            raw_d_x=data_feeder.raw_d.x, #instances from the \"d\" set\n",
    "            raw_d_L=data_feeder.raw_d.L) #labels from the \"d\" set\n",
    "\n",
    "float_formatter = lambda x: \"%.3f\" % x # Output 3 digits after decimal point in numpy arrays\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41b23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Run mode is ' + mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd904b0",
   "metadata": {},
   "source": [
    "### **Train and Test on the hls object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'f_d':\n",
    "    print('training f on d')\n",
    "    hls.train.train_f_on_d(data_feeder, f_d_epochs)\n",
    "elif mode[:4]!=\"test\":\n",
    "    print(mode+\" training started\")\n",
    "    hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type=mode)\n",
    "    print(mode+\" training ended\")\n",
    "elif mode == 'test_f':\n",
    "    print('Running test_f')\n",
    "    hls.test.test_f(data_feeder, log_output=True, \n",
    "                    save_filename=f_infer_out_pickle, \n",
    "                    use_joint_f_w=use_joint_f_w)\n",
    "elif mode == 'test_w': # use only if train_mode = implication or train_mode = pr_loss\n",
    "    print('Running test_w')\n",
    "    hls.test.test_w(data_feeder, log_output=True, save_filename=w_infer_out_pickle+\"_test\")\n",
    "elif mode == 'test_all': # use only if train_mode = implication or train_mode = pr_loss\n",
    "    print('Running all tests')\n",
    "    print('\\ninference on f network ...\\n')\n",
    "    hls.test.test_f(data_feeder, log_output=True, \n",
    "                    save_filename=f_infer_out_pickle,\n",
    "                    use_joint_f_w=use_joint_f_w)\n",
    "    print('\\ninference on w network...')\n",
    "    print('we only test on instances covered by atleast one rule\\n')\n",
    "    hls.test.test_w(data_feeder, log_output=True, save_filename=w_infer_out_pickle+\"_test\")\n",
    "else:\n",
    "    assert not \"Invalid mode string: %s\" % mode\n",
    "\n",
    "sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
