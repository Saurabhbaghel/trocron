{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fde778",
   "metadata": {},
   "source": [
    "# ***Subset selection:***\n",
    "**The paper, documentation, colab notebook can be found here:** [Paper](https://arxiv.org/abs/2008.09887), [Documentation](https://spear-decile.readthedocs.io/en/latest/#subset-selection), [Colab](https://colab.research.google.com/drive/1HqkqQ8ytWjP9on3du-vVB07IQvo8Li3W?ts=60ce20fe)(Subset selection section can be found quite below in colab notebook)\n",
    "\n",
    "For subset selection, we use FacilityLocation from the [submodlib](https://github.com/decile-team/submodlib) library which is also provided by [DECILE](https://decile.org/) for submodular optimization.\n",
    "\n",
    "<p>This notebook aims at demonstrating the use cases for the functions in spear library for subset selection. Subset selection is selecting a small subset of unlabeled data(or the data labeled by LFs, in case of supervised subset selection) so that it can be labeled and use that small labeled data(the L dataset) for effective training of <b>JL algorithm</b>(Cage algorithm doesn't need labeled data). Finding the best subset makes best use of the labeling efforts. Note that for this notebook demo, we need data generated from the first half(labeling part) of sms_jl.ipynb.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f499ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "User don't need to include this cell to use the package\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c3d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d757bf",
   "metadata": {},
   "source": [
    "### **Random subset selection**\n",
    "Here we select a random subset of instances to label. We need number of instances available and number of instances we intend to label to get a sorted numpy array of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356ba6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices selected by rand_subset:  [ 0  3  4 10 12]\n",
      "return type of rand_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.jl import rand_subset\n",
    "\n",
    "indices = rand_subset(n_all = 20, n_instances = 5) #select 5 instances from a total of 20 instances\n",
    "print(\"indices selected by rand_subset: \", indices)\n",
    "print(\"return type of rand_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cecb70",
   "metadata": {},
   "source": [
    "### **Unsupervised subset selection**\n",
    "Here we select a unsupervised subset(for more on this, please refer [here](https://arxiv.org/abs/2008.09887) ) of instances to label. We need feature matrix(of shape (num_instaces, num_features)) and number of instances we intend to label and we get a sorted numpy array of indices. For any other arguments to unsup_subset(or to sup_subset_indices or sup_subset_save_files) please refer documentation.\n",
    "<p>For this let's first get some data(feature matrix), say from sms_pickle_U.pkl(in data_pipeline folder). For more on this pickle file, please refer the other notebook named sms_jl.ipynb</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449ec58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_U shape:  (4500, 1024)\n",
      "x_U type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "data_U = get_data(U_path_pkl, check_shapes=True)\n",
    "x_U = data_U[0] #the feature matrix\n",
    "print(\"x_U shape: \", x_U.shape)\n",
    "print(\"x_U type: \", type(x_U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c16aa",
   "metadata": {},
   "source": [
    "Now that we have feature matrix, let's select the indices to label from it. After labeling(through a trustable means/SMEs) those instances, whose indices(index with respect to feature matrix) are given by the following function, one can pass them as gold_labels to the PreLabels class in the process for labeling the subset-selected data and forming a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6972ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 indices given by unsup_subset:  [ 455  659  806  985 1036 1438 2092 2197 2277 2283]\n",
      "return type of unsup_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.jl import unsup_subset\n",
    "\n",
    "indices = unsup_subset(x_train = x_U, n_unsup = 20)\n",
    "print(\"first 10 indices given by unsup_subset: \", indices[:10])\n",
    "print(\"return type of unsup_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7f408",
   "metadata": {},
   "source": [
    "### **Supervised subset selection**\n",
    "Here we select a supervised subset(for more on this, please refer [here](https://arxiv.org/abs/2008.09887) ) of instances to label. We need \n",
    "* path to json file having information about classes\n",
    "* path to pickle file generated by feature matrix after labeling using LFs\n",
    "* number of instances we intend to label\n",
    "\n",
    "<p>we get a sorted numpy array of indices.</p>\n",
    "<p>For this let's use sms_json.json, sms_pickle_U.pkl(in data_pipeline folder). For more on this json/pickle file, please refer the other notebook named sms_cage_jl.ipynb</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8659db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 indices given by sup_subset:  [1632 1848 3284 4403 4404 4405 4406 4407 4408 4409]\n",
      "return type of sup_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.jl import sup_subset_indices\n",
    "\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "indices = sup_subset_indices(path_json = path_json, path_pkl = U_path_pkl, n_sup = 100, qc = 0.85)\n",
    "\n",
    "print(\"first 10 indices given by sup_subset: \", indices[:10])\n",
    "print(\"return type of sup_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e8d97",
   "metadata": {},
   "source": [
    "Instead of just getting indices to already labeled data(stored in pickle format, using LFs), we also provide the following utility to split the input pickle file and save two pickle files on the basis of subset selection. Make sure that the directory of the files(path_save_L and path_save_U) exists. Note that any existing contents in these pickle files will be erased. You can still get the return value of subset-selected indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd0353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 indices given by sup_subset:  [1632 1848 3284 4403 4404 4405 4406 4407 4408 4409]\n",
      "return type of sup_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.jl import sup_subset_save_files\n",
    "\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "path_save_L = 'data_pipeline/JL/sup_subset_L.pkl'\n",
    "path_save_U = 'data_pipeline/JL/sup_subset_U.pkl'\n",
    "\n",
    "indices = sup_subset_save_files(path_json = path_json, path_pkl = U_path_pkl, path_save_L = path_save_L, \\\n",
    "                             path_save_U = path_save_U, n_sup = 100, qc = 0.85)\n",
    "\n",
    "print(\"first 10 indices given by sup_subset: \", indices[:10])\n",
    "print(\"return type of sup_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3800e",
   "metadata": {},
   "source": [
    "### **Inserting true labels into pickle files**\n",
    "Now after doing supervised subset selection, say we get two pickle files path_save_L and path_save_U. Now say you labeled the instances of path_save_L and want to insert them into pickle file. So here, instead of going over the process of generating pickle through PreLabels again, you can use the following function to create a new pickle file, which now contain true labels, using path_save_L pickle file. There is no return value to this function.\n",
    "<p>Make sure that path_save file, the pickle file path that is to be formed with the data in path_save_L file and true labels, is in an existing directory. Note that any existing contents in this pickle file(path_save) will be erased.</p>\n",
    "<p>Note that one can pass same file to path, path_save and path arguments, in which case the true labels numpy array is just replaced with what user provides in labels argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68700e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.jl import insert_true_labels\n",
    "\n",
    "path_save_L = 'data_pipeline/JL/sup_subset_L.pkl'\n",
    "path_save_labeled = 'data_pipeline/JL/sup_subset_labeled_L.pkl'\n",
    "labels = np.random.randint(0,2,[100, 1])\n",
    "'''\n",
    "Above is just a random association of labels used for demo. In real time user has to label the instances in\n",
    "path_save_L with a trustable means/SMEs and use it here.\n",
    "\n",
    "Note that the shape of labels is (num_instances, 1) and just for reference, feature_matrix(the first element\n",
    "in pickle file) in path_save_L has shape (num_instances, num_features).\n",
    "'''\n",
    "insert_true_labels(path = path_save_L, path_save = path_save_labeled, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd2f3f",
   "metadata": {},
   "source": [
    "A similar function as insert_true_labels called replace_in_pkl is also made available to make changes to pickle file. replace_in_pkl usage is demonstrated below. Make sure that path_save, the pickle file path that is to be formed with the data in path file and a new numpy array, is in an existing directory. Note that any existing contents in this pickle file(path_save) will be erased. There is no return value for this function too.\n",
    "<p>Note that one can pass same file to path, path_save and path arguments, in which case the intended numpy array is just replaced with what user provides in np_array argument.</p>\n",
    "<p>It is highly advised to use insert_true_labels function for the purpose of inserting the labels since it does some other necessary changes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc5e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.jl import replace_in_pkl\n",
    "\n",
    "path_labeled = 'data_pipeline/JL/sup_subset_labeled_L.pkl' # this is the previously used path, path_save_labeled\n",
    "path_save_altered = 'data_pipeline/JL/sup_subset_altered_L.pkl'\n",
    "np_array = np.random.randint(0,2,[100, 1]) #we are just replacing the labels we inserted before\n",
    "index = 3 \n",
    "'''\n",
    "index refers to the element we intend to replace. Refer documentaion(specifically \n",
    "spear.utils.data_editor.get_data) to understand which numpy array an index value\n",
    "maps to(order the contents of pickle file from 0 to 8). Index should be in range [0,8].\n",
    "'''\n",
    "\n",
    "replace_in_pkl(path = path_labeled, path_save = path_save_altered, np_array = np_array, index = index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee87f1",
   "metadata": {},
   "source": [
    "### **Demonstrating the use of labeled subset-selected data**\n",
    "Now that we have our subset(labeled) in path_save_labeled, lets see a use case by calling a member function of JL class using path_save_labeled as our path to L data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82c3681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:56<02:58,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 24\tbest_epoch: 13\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.6037735849056604\tbest_fm_val_score:0.6808510638297872\n",
      "best_gm_test_score:0.5581395348837209\tbest_fm_test_score:0.5818181818181818\n",
      "best_gm_test_precision:0.4\tbest_fm_test_precision:0.4247787610619469\n",
      "best_gm_test_recall:0.9230769230769231\tbest_fm_test_recall:0.9230769230769231\n",
      "probs_fm shape:  (4400, 2)\n",
      "probs_gm shape:  (4400, 2)\n"
     ]
    }
   ],
   "source": [
    "from spear.jl import JL\n",
    "\n",
    "n_lfs = 16\n",
    "n_features = 1024\n",
    "n_hidden = 512\n",
    "feature_model = 'nn'\n",
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, feature_model = feature_model, \\\n",
    "        n_hidden = n_hidden)\n",
    "\n",
    "L_path_pkl = path_save_labeled #Labeled data - have true labels\n",
    "'''\n",
    "Note that I saved random labels, in file path_save_labeled, as true labels which are \n",
    "supposed to be labeled by a trustable means/SMEs. Hence the accuracies below can be small.\n",
    "'''\n",
    "U_path_pkl = path_save_U #unlabelled data - don't have true labels\n",
    "V_path_pkl = 'data_pipeline/JL/sms_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/JL/sms_pickle_T.pkl' #test data - have true labels\n",
    "log_path_jl_1 = 'log/JL/jl_log_1.txt'\n",
    "loss_func_mask = [1,1,1,1,1,1,1] \n",
    "batch_size = 150\n",
    "lr_fm = 0.0005\n",
    "lr_gm = 0.01\n",
    "use_accuracy_score = False\n",
    "\n",
    "probs_fm, probs_gm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)\n",
    "print(\"probs_gm shape: \", probs_gm.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
