{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***End to End tutorial for SMS_SPAM labeling using SPEAR(Cage and JL):***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Defining an Enum to hold labels:***\n",
    "### **Representation of class Labels**\n",
    "\n",
    "<p>All the class labels for which we define labeling functions are encoded in enum and utilized in our next tasks. Make sure not to define an Abstain(Labeling function(LF) not deciding anything) class inside this Enum, instead use the Abstain object as used later in LF section.</p>\n",
    "\n",
    "<p>SPAM dataset contains 2 classes i.e <b>HAM</b> and <b>SPAM</b>. Note that the numbers we associate can be anything but it is suggested to use a continuous numbers from 0 to number_of_classes-1</p>\n",
    "\n",
    "<p><b>**Note that even though this example is a binary classification, this(SPEAR) library supports multi-label classification**</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# enum to hold the class labels\n",
    "class ClassLabels(enum.Enum):\n",
    "    SPAM = 1\n",
    "    HAM = 0\n",
    "\n",
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Defining preprocessors, continuous_scorers, labeling functions:***\n",
    "During labeling the unlabelled data we lookup for few keywords to assign a class SMS.\n",
    "\n",
    "<b>Example</b> : *If a message contains apply or buy in it then most probably the message is spam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigWord1 = {\"free\",\"credit\",\"cheap\",\"apply\",\"buy\",\"attention\",\"shop\",\"sex\",\"soon\",\"now\",\"spam\"}\n",
    "trigWord2 = {\"gift\",\"click\",\"new\",\"online\",\"discount\",\"earn\",\"miss\",\"hesitate\",\"exclusive\",\"urgent\"}\n",
    "trigWord3 = {\"cash\",\"refund\",\"insurance\",\"money\",\"guaranteed\",\"save\",\"win\",\"teen\",\"weight\",\"hair\"}\n",
    "notFreeWords = {\"toll\",\"Toll\",\"freely\",\"call\",\"meet\",\"talk\",\"feedback\"}\n",
    "notFreeSubstring = {\"not free\",\"you are\",\"when\",\"wen\"}\n",
    "firstAndSecondPersonWords = {\"I\",\"i\",\"u\",\"you\",\"ur\",\"your\",\"our\",\"we\",\"us\",\"youre\"}\n",
    "thirdPersonWords = {\"He\",\"he\",\"She\",\"she\",\"they\",\"They\",\"Them\",\"them\",\"their\",\"Their\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declaration of a simple preprocessor function**\n",
    "\n",
    "\n",
    "For most of the tasks in NLP, computer vivsion instead of using the raw datapoint we preprocess the datapoint and then label it. Preprocessor functions are used to preprocess an instance before labeling it. We use **`@preprocessor(name,resources)`** decorator to declare a function as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "\n",
    "@preprocessor(name = \"LOWER_CASE\")\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "lower = convert_to_lower(\"RED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some Labeling function(LF) definitions**\n",
    "Below are some examples on how to define LFs and continuous LFs(CLFs). To get the continuous score for a CLF, we need to define a function with continuous_scorer decorator(just like labeling_function decorator) and pass it to a CLF as displayed below. Also note how the continuous score can be used in CLF. Note that the word_similarity is the function with continuous_scorer decorator and is written in con_scorer file(this file is not a part of package) in same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import labeling_function, ABSTAIN\n",
    "\n",
    "from helper.con_scorer import word_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "@preprocessor()\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF1(c,**kwargs):    \n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF2(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def LF3(c,**kwargs):\n",
    "    if len(kwargs[\"keywords\"].intersection(c.split())) > 0:\n",
    "        return ClassLabels.SPAM \n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF4(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF5(c,**kwargs):\n",
    "    for pattern in kwargs[\"keywords\"]:    \n",
    "        if \"free\" in c.split() and re.search(pattern,c, flags= re.I):\n",
    "            return ClassLabels.HAM\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function(resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF6(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def LF7(c,**kwargs):\n",
    "    if \"free\" in c.split() and len(kwargs[\"keywords\"].intersection(c.split()))>0:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(label=ClassLabels.SPAM)\n",
    "def LF8(c,**kwargs):\n",
    "    if (sum(1 for ch in c if ch.isupper()) > 6):\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord1),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF1(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord2),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF2(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=trigWord3),pre=[convert_to_lower],label=ClassLabels.SPAM)\n",
    "def CLF3(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF4(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=notFreeSubstring),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF5(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=firstAndSecondPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF6(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=word_similarity,resources=dict(keywords=thirdPersonWords),pre=[convert_to_lower],label=ClassLabels.HAM)\n",
    "def CLF7(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "@labeling_function(cont_scorer=lambda x: 1-np.exp(float(-(sum(1 for ch in x if ch.isupper()))/2)),label=ClassLabels.SPAM)\n",
    "def CLF8(c,**kwargs):\n",
    "    if kwargs[\"continuous_score\"] >= THRESHOLD:\n",
    "        return ClassLabels.SPAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Accumulating all LFs into rules, an LFset(a class) object:***\n",
    "### **Importing LFSet and passing LFs we defined, to that class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import LFSet\n",
    "\n",
    "LFS = [LF1,\n",
    "    LF2,\n",
    "    LF3,\n",
    "    LF4,\n",
    "    LF5,\n",
    "    LF6,\n",
    "    LF7,\n",
    "    LF8,\n",
    "    CLF1,\n",
    "    CLF2,\n",
    "    CLF3,\n",
    "    CLF4,\n",
    "    CLF5,\n",
    "    CLF6,\n",
    "    CLF7,\n",
    "    CLF8\n",
    "      ]\n",
    "\n",
    "rules = LFSet(\"SPAM_LF\")\n",
    "rules.add_lf_list(LFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Loading data:***\n",
    "### **Load the data: X, X_feats, Y**\n",
    "<p>Note that the utils below is not a part of package but is used to load the necessary data. User have to use some means(which doesn't matter) to load his data. X is the raw data that is to be passed to LFs, X_feats is a numpy array of shape (num_instances, num_features) and Y are true labels(if available).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.utils import load_data_to_numpy, get_various_data\n",
    "\n",
    "X, X_feats, Y = load_data_to_numpy()\n",
    "\n",
    "validation_size = 100\n",
    "test_size = 400\n",
    "L_size = 100\n",
    "U_size = 4500\n",
    "n_lfs = len(rules.get_lfs())\n",
    "\n",
    "X_V, Y_V, X_feats_V,_, X_T, Y_T, X_feats_T,_, X_L, Y_L, X_feats_L,_, X_U, X_feats_U,_ = get_various_data(X, Y,\\\n",
    "    X_feats, n_lfs, validation_size, test_size, L_size, U_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Labeling data:***\n",
    "### **Paths**\n",
    "* path_json: path to json file generated by PreLabels\n",
    "* V_path_pkl: path to pkl file generated by PreLabels containing the validation data with true labels\n",
    "* L_path_pkl: path to pkl file generated by PreLabels containing the labeled data with true labels\n",
    "* T_path_pkl: path to pkl file generated by PreLabels containing the test data with true labels\n",
    "* U_path_pkl: path to pkl file generated by PreLabels containing the unlabelled data without true labels\n",
    "* log_path: path to save the log which is generated during the algorithm\n",
    "\n",
    "<p>Difference between test and labeled data is that labeled data may be used in the algorithm(JL uses it while Cage doesn't) but test data isn't. Make sure to have the pickle files <b>EMPTY</b> ie, it should not any data inside it before passing to .generate_pickle() member function of PreLabels</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'data_pipeline/sms_json.json'\n",
    "V_path_pkl = 'data_pipeline/sms_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/sms_pickle_T.pkl' #test data - have true labels\n",
    "L_path_pkl = 'data_pipeline/sms_pickle_L.pkl' #Labeled data - have true labels\n",
    "U_path_pkl = 'data_pipeline/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "\n",
    "log_path_cage_1 = 'log/cage_log_1.txt' #cage is an algorithm, can be found below\n",
    "log_path_jl_1 = 'log/jl_log_1.txt' #jl is an algorithm, can be found below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing PreLabels class and using it to label data**\n",
    "Json file should be generated only once as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.19it/s]\n",
      "100%|██████████| 400/400 [00:27<00:00, 14.81it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.05it/s]\n",
      "100%|██████████| 4500/4500 [05:23<00:00, 13.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import PreLabels\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_V,\n",
    "                               gold_labels=Y_V,\n",
    "                               data_feats=X_feats_V,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(V_path_pkl)\n",
    "sms_noisy_labels.generate_json(path_json) #generating json files once is enough\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_T,\n",
    "                               gold_labels=Y_T,\n",
    "                               data_feats=X_feats_T,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(T_path_pkl)\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_L,\n",
    "                               gold_labels=Y_L,\n",
    "                               data_feats=X_feats_L,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(L_path_pkl)\n",
    "\n",
    "sms_noisy_labels = PreLabels(name=\"sms\",\n",
    "                               data=X_U,\n",
    "                               rules=rules,\n",
    "                               data_feats=X_feats_U,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=2)\n",
    "sms_noisy_labels.generate_pickle(U_path_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Accessing labeled data:***\n",
    "### **Importing and the use of get_data and get_classes**\n",
    "<p>These functions can be used to extract data from pickle files and json file respectively. Note that these are the files generated using PreLabels.</p>\n",
    "<p>For detailed contents of output, please refer documentation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data list:  10\n",
      "Shape of feature matrix:  (4500, 1024)\n",
      "Shape of labels matrix:  (4500, 16)\n",
      "Shape of continuous scores matrix :  (4500, 16)\n",
      "Total number of classes:  2\n",
      "Classes dictionary in json file(modified to have integer keys):  {1: 'SPAM', 0: 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "data_U = get_data(path = U_path_pkl, check_shapes=True)\n",
    "#check_shapes being True(above), asserts for relative shapes of arrays in pickle file\n",
    "print(\"Number of elements in data list: \", len(data_U))\n",
    "print(\"Shape of feature matrix: \", data_U[0].shape)\n",
    "print(\"Shape of labels matrix: \", data_U[1].shape)\n",
    "print(\"Shape of continuous scores matrix : \", data_U[6].shape)\n",
    "print(\"Total number of classes: \", data_U[9])\n",
    "\n",
    "classes = get_classes(path = path_json)\n",
    "print(\"Classes dictionary in json file(modified to have integer keys): \", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Cage Algorithm:***\n",
    "### **Importing Cage class (the algorithm) and declaring an object of it**\n",
    "Cage algorithm needs only the pickle file(with labels given by LFs using PreLabels class) with unlabelled data(the data without true/gold labels) and it will predict the labels of this data. An optinal test data(which has true/gold labels) can also passed to get a log information of accuracies. \n",
    "<p><b>Note:</b> Multiple calls to fit_* functions will train parameters continuously ie, parameters are not reinitialised in fit_* functions. So, to train large data, one can call fit_* functions repeatedly on smaller chunks. Also, in order to perform multiple runs over the algorithm, one need to reinitialise paramters(by creating an object of Cage) at the start of each run.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.Cage import Cage\n",
    "\n",
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict_proba function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class. For more details about arguments, please refer documentation; same should be the case for any of the member functions used from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.815\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5747126436781609\n",
      "probs shape:  (4500, 2)\n",
      "labels shape:  (4500,)\n"
     ]
    }
   ],
   "source": [
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "\n",
    "probs = cage.fit_and_predict_proba(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                                   qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01)\n",
    "labels = np.argmax(probs, 1)\n",
    "print(\"probs shape: \", probs.shape)\n",
    "print(\"labels shape: \",labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(because need_strings is False), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.815\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5747126436781609\n",
      "labels shape:  (4500,)\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "\n",
    "labels = cage.fit_and_predict(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                              qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01, \\\n",
    "                              need_strings = False)\n",
    "\n",
    "print(\"labels shape: \", labels.shape)\n",
    "print(type(labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing strings(because need_strings is True), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_test_accuracy_score: 0.815\n",
      "test_average_metric: binary\tfinal_test_f1_score: 0.5747126436781609\n",
      "labels_strings shape:  (4500,)\n",
      "<class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "cage = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "\n",
    "labels_strings = cage.fit_and_predict(path_pkl = U_path_pkl, path_test = T_path_pkl, path_log = log_path_cage_1, \\\n",
    "                              qt = 0.9, qc = 0.85, metric_avg = ['binary'], n_epochs = 200, lr = 0.01, \\\n",
    "                              need_strings = True)\n",
    "\n",
    "print(\"labels_strings shape: \", labels_strings.shape)\n",
    "print(type(labels_strings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save parameters**\n",
    "<p>Make sure the pickle you are passing here is <b>EMPTY</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage.save_params(save_path = 'params/sms_cage_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_2 = Cage(path_json = path_json, n_lfs = n_lfs)\n",
    "cage_2.load_params(load_path = 'params/sms_cage_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict_proba function of Cage class**\n",
    "The output(probs_test) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in Cage class. Hope you have loaded parameters.\n",
      "probs_test shape:  (400, 2)\n"
     ]
    }
   ],
   "source": [
    "probs_test = cage_2.predict_proba(path_test = T_path_pkl, qc = 0.85) \n",
    "#NEED NOT use the same test data(above) used in Cage class before.\n",
    "print(\"probs_test shape: \",probs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict function of Cage class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(strings) if need_strings is Flase(True), having the classes of each instance. Just the use case with need_strings as False is displayed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in Cage class. Hope you have loaded parameters.\n",
      "labels_test shape:  (400,)\n",
      "accuracy_score:  0.815\n",
      "f1_score:  0.5747126436781609\n"
     ]
    }
   ],
   "source": [
    "labels_test = cage_2.predict(path_test = T_path_pkl, qc = 0.85, need_strings = False)\n",
    "print(\"labels_test shape: \", labels_test.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#Y_T is true labels of test data, type is numpy array of shape (num_instances,)\n",
    "print(\"accuracy_score: \", accuracy_score(Y_T, labels_test))\n",
    "print(\"f1_score: \", f1_score(Y_T, labels_test, average = 'binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting numpy array of integers to enums**\n",
    "The below utility from spear can help convert return values of predict(obtained when need_strings is Flase) to a numpy array of enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enum 'ClassLabels'>\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_enum\n",
    "\n",
    "labels_test_enum = get_enum(np_array = labels_test, enm = ClassLabels) \n",
    "#the second argument is the Enum class defined at beginning\n",
    "print(type(labels_test_enum[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Joint Learning(JL) Algorithm:***\n",
    "## **Importing JL class (the algorithm) and declaring an object of it**\n",
    "JL algoritm needs the four types of data:(all this data should be labeled using LFs via PreLabels class)\n",
    "* Unlabeled data(doesn't have true/gold labels)\n",
    "* labeled data(have true/gold labels)\n",
    "* validation data(have true/gold labels)\n",
    "* test data(have true/gold labels)\n",
    "\n",
    "<p>All this data is compulsory for training(passed in fit_and_predict functions). Note that the amount of labeled or validation data can be small, for example they can be of the order of 100 each. Also refer subset selection to find the subset of the data, that is available with you, to label(using a trustable means) and use it as 'labeled data' so that the data complements the LFs.</p>\n",
    "<p>The member functions of JL can be choosen to return fm(feature model) or gm(graphical model) predictions. It is highly advised to use the predictions of fm.</p>\n",
    "<p><b>Note:</b> Multiple calls to fit_* functions will train parameters continuously ie, parameters are not reinitialised in fit_* functions. So, to train large data, one can call fit_* functions repeatedly on smaller chunks. Also, in order to perform multiple runs over the algorithm, one need to reinitialise paramters(by creating an object of JL) at the start of each run.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.JL import JL\n",
    "\n",
    "n_features = 1024\n",
    "n_hidden = 512\n",
    "feature_model = 'nn'\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict_proba function of JL class with two return values**\n",
    "Here return_gm argument is True which returns predictions from graphical model(Cage) along with feature model. Also note that here test data(path_T) is compulsory and metric_avg is just one value(instead of list as in Cage). The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 19\tbest_epoch: 8\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.5581395348837209\tbest_fm_val_score:0.7499999999999999\n",
      "best_gm_test_score:0.6025641025641025\tbest_fm_test_score:0.7999999999999999\n",
      "best_gm_test_precision:0.46078431372549017\tbest_fm_test_precision:0.7272727272727273\n",
      "best_gm_test_recall:0.8703703703703703\tbest_fm_test_recall:0.8888888888888888\n",
      "probs_fm shape:  (4500, 2)\n",
      "probs_gm shape:  (4500, 2)\n"
     ]
    }
   ],
   "source": [
    "loss_func_mask = [1,1,1,1,1,1,1] \n",
    "'''\n",
    "One can keep 0s in places where he don't want the specific loss function to be part\n",
    "the final loss function used in training. Refer documentation(spear.JL.core.JL) to understand\n",
    "the which index of loss_func_mask refers to what loss function.\n",
    "Note: the loss_func_mask may not be the optimal mask for sms dataset.\n",
    "'''\n",
    "batch_size = 150\n",
    "lr_fm = 0.0005\n",
    "lr_gm = 0.01\n",
    "use_accuracy_score = False\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "probs_fm, probs_gm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)\n",
    "print(\"probs_gm shape: \", probs_gm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict_proba function of JL class with one return value**\n",
    "Here return_gm argument is False which returns predictions only from feature model. The output(probs) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 27\tbest_epoch: 16\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.5454545454545454\tbest_fm_val_score:0.631578947368421\n",
      "best_gm_test_score:0.5949367088607594\tbest_fm_test_score:0.676056338028169\n",
      "best_gm_test_precision:0.4519230769230769\tbest_fm_test_precision:0.5454545454545454\n",
      "best_gm_test_recall:0.8703703703703703\tbest_fm_test_recall:0.8888888888888888\n",
      "probs_fm shape:  (4500, 2)\n"
     ]
    }
   ],
   "source": [
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "probs_fm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = False, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of JL class**\n",
    "Here return_gm argument is True. The output(probs) is a numpy matrix of shape (num_instances,) containing integers(because need_strings is False), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 27\tbest_epoch: 16\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.5581395348837209\tbest_fm_val_score:0.7058823529411764\n",
      "best_gm_test_score:0.5911949685534591\tbest_fm_test_score:0.7868852459016393\n",
      "best_gm_test_precision:0.44761904761904764\tbest_fm_test_precision:0.7058823529411765\n",
      "best_gm_test_recall:0.8703703703703703\tbest_fm_test_recall:0.8888888888888888\n",
      "labels_fm shape:  (4500,)\n",
      "labels_gm shape:  (4500,)\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "labels_fm, labels_gm = jl.fit_and_predict(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary', \\\n",
    "    need_strings = False)\n",
    "\n",
    "print(\"labels_fm shape: \", labels_fm.shape)\n",
    "print(\"labels_gm shape: \", labels_gm.shape)\n",
    "print(type(labels_fm[0]))\n",
    "print(type(labels_gm[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **fit_and_predict function of JL class**\n",
    "Here return_gm argument is True. The output(probs) is a numpy matrix of shape (num_instances,) containing strings(because need_strings is True), having the classes of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 25\tbest_epoch: 14\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.5714285714285715\tbest_fm_val_score:0.7272727272727273\n",
      "best_gm_test_score:0.6025641025641025\tbest_fm_test_score:0.8235294117647058\n",
      "best_gm_test_precision:0.46078431372549017\tbest_fm_test_precision:0.7538461538461538\n",
      "best_gm_test_recall:0.8703703703703703\tbest_fm_test_recall:0.9074074074074074\n",
      "labels_fm shape:  (4500,)\n",
      "labels_gm shape:  (4500,)\n",
      "<class 'numpy.str_'>\n",
      "<class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "labels_fm, labels_gm = jl.fit_and_predict(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary', \\\n",
    "    need_strings = True)\n",
    "\n",
    "print(\"labels_fm shape: \", labels_fm.shape)\n",
    "print(\"labels_gm shape: \", labels_gm.shape)\n",
    "print(type(labels_fm[0]))\n",
    "print(type(labels_gm[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save parameters**\n",
    "<p>Make sure the pickle you are passing here is <b>EMPTY</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.save_params(save_path = 'params/sms_jl_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl_2 = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden =  n_hidden, \\\n",
    "          feature_model = feature_model)\n",
    "jl_2.load_params(load_path = 'params/sms_jl_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict_fm/gm_proba functions of JL class**\n",
    "The output(probs_test) is a numpy matrix of shape (num_instances, num_classes) having the probability of a particular instance being that class.\n",
    "<p>Note that predict_fm_proba takes feature matrix(can also be obtained from pickle file using get_data()) as argument while predict_gm_proba takes pickle file(containing labels given by LFs) as argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in JL class. Hope you have loaded parameters.\n",
      "probs_fm_test shape:  (400, 2)\n",
      "probs_gm_test shape:  (400, 2)\n"
     ]
    }
   ],
   "source": [
    "probs_fm_test = jl_2.predict_fm_proba(x_test = X_feats_T)\n",
    "probs_gm_test = jl_2.predict_gm_proba(path_test = T_path_pkl, qc = 0.85)\n",
    "\n",
    "print(\"probs_fm_test shape: \", probs_fm_test.shape)\n",
    "print(\"probs_gm_test shape: \", probs_gm_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **predict_fm/gm functions of JL class**\n",
    "The output(probs) is a numpy matrix of shape (num_instances,) containing integers(strings) if need_strings is Flase(True), having the classes of each instance. Just the use case with need_strings as False is displayed here. \n",
    "<p>Note that predict_fm takes feature matrix(can also be obtained from pickle file using get_data()) as argument while predict_gm takes pickle file(containing labels given by LFs) as argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Predict is used before training any paramters in JL class. Hope you have loaded parameters.\n",
      "labels_fm_test shape:  (400,)\n",
      "labels_gm_test shape:  (400,)\n",
      "accuracy_score of gm:  0.815 | fm:  0.9475\n",
      "f1_score of gm:  0.5595238095238095 | fm:  0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "labels_fm_test = jl_2.predict_fm(x_test = X_feats_T, need_strings=False)\n",
    "labels_gm_test = jl_2.predict_gm(path_test = T_path_pkl, qc = 0.85, need_strings=False)\n",
    "\n",
    "print(\"labels_fm_test shape: \", labels_fm_test.shape)\n",
    "print(\"labels_gm_test shape: \", labels_gm_test.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#Y_T is true labels of test data, type is numpy array of shape (num_instances,)\n",
    "print(\"accuracy_score of gm: \", accuracy_score(Y_T, labels_gm_test), \"| fm: \", accuracy_score(Y_T, labels_fm_test))\n",
    "print(\"f1_score of gm: \", f1_score(Y_T, labels_gm_test, average = 'binary'), \"| fm: \", f1_score(Y_T, labels_fm_test, average = 'binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting numpy array of integers to enums**\n",
    "The below utility from spear can help convert return values of predict_fm, predict_gm(obtained when need_strings is Flase) to a numpy array of enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enum 'ClassLabels'>\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_enum\n",
    "\n",
    "probs_fm_test_enum = get_enum(np_array = labels_fm_test, enm = ClassLabels) \n",
    "#the second argument is the Enum class defined at beginning\n",
    "print(type(probs_fm_test_enum[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
